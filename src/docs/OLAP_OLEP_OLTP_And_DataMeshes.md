## OLAP, OLEP, OLTP and Data Meshes

### Understanding the Differences between an OLAP database and a OLTP database. 

An OLAP (Online Analytical Processing) database is optimized for queries and reporting.  OLAP DBs usually arrange data multi-dimensionally in a cube like format.  There are two primary implementations Star or Snowlfake schema.  

A Star schema is intended to minimize the number of large tables by arranging them in a star pattern, usually around a single Fact or Statistics table.  Fact tables hold data like totals, averages, counts (similar to aggregations) and has Foreign keys to tables on each point of the star.  Each of the points to the star represents a table with data that describes the domain objects or entities of the use case being modeled.  A star schema is optimized for Reads, entity/non-fact tables are denormalized but designed to be small so performance doesn't become a problem and they aren't subject to frequent change.  These properties make it great for building a data warehouse with lots of data that will be heavily queried for facts using reports.  On the other hand if a dimension table does become larger queries can slow down, dimensions can have data that cross-cuts causing duplicates, and the quantitative data in the fact table can be complex to calculate/aggregate.

A Snowflake schema extends the star schema where each dimension table is broken down into sub dimensions.  This makes the data model significantly more complex than a star schema, however it can make it much easier for analysts to use.  They are more normalized causing less duplication and better storage efficiency, but may require more complex joins and queries.  This strict data model can be tough to setup and maintain, but can usually be applied to many data warehousing use cases ubiquitously.

Data is usually loaded in batches and the amount of data that could be queried can be quite vast.

The above schemas have their pros and cons, but they share a common goal, fast data retrieval.  Making them great for reporting use cases on very large datasets like a data warehouse.

OLTP (Online Transaction Processing) Databases conversely are optimized for Write heavy workloads and processing high volumes transactions that require strong consistency and ensure integrity.  The schema for OLTP systems are single dimensional and focus on a single data aspect.  Table rows represent entity instances and each column an attribute of that entity.  OLTP systems usually deal with much less data than an OLAP system and can be either use a normalized or denormalized model.

### My Experience and Examples

Most of my experience has been with OLTP relational and non-relational databases where data like user clicks, customer account updates, access keys, token requests, and similar data were stored from customers environments or their interactions with a publicly available robust UI/API.  However, I have experienced the trade-off's first hand of using a relational OLTP system for Read/Reporting heavy workloads at scale with LogRhythm's on-prem data product.  We were able to combat scaling and partition maintenance issues by switching to a non-relational backend for log data: Elasticsearch/OpenSearch.  This allowed us to scale our product from a 90k Message Per Second capacity up to 300k MPS over time.  These clusters would get very large over time due to the time-series nature of the data, but they also support different data tiers.  For instance, hot, warm, and cold storage.  Where access to the data would slow down as the associated temperature of the storage tier decreased.  Cold storage was great for long term archival and could be hosted on cheaper storage engines like S3, but require more processing by the DBMS to re-hydrate and uncompress in order to Read for analysis.  This tiered storage scheme allowed us to maintain heavy writes and a path to satisfy reports with up to 365 days of TTL(s) and beyond while keeping access to Hot storage quick for intensive Reads and Writes.  Later in my career while re-building LogRhythm's core product we designed a data mesh.  This allowed us to provide purpose built OLTP and OLEP (Online Event Processing) self-service databases in postgres for each isolated domain service to be considered part of each microservice's bounded context.  Or datastores could be created and scaled centrally to allow the creation of data lakes or event buses.  Or automatically provisioned streaming/batch analytics engines like Flink to isolate and independently scale on a per customer/tenant basis.  In my personal opinion data meshes are far superior to a single data warehouse with an overloaded and over generalized data schema, but I would not be surprised to find OLAP schemas being rebuilt iteratively by a data mesh on a per use case basis over time.

Since originally authoring this I have now greatly expanded much of my experience with OLAP databases implemented in Snowflake, Redshift, and others during my time with Select Star.  This was a great opportunity to see a myriad of data models out in the field and glean insights from architectures from connected lineages to reporting services.  That being said, I still very much hold to my previous assertions that a properly designed and implemented data mesh architecture can create a much more robust, maintainable, and scalable architecture that combines the benefits of all three relational data models.